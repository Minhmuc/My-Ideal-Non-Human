from core.llm_interface import ask_llm_with_memory, provide_data_via_chat
from core.memory import ConversationBufferMemory
from core.prompt_engineering import exit_intent_confidence
from core.prompts import get_prompt,exit_prompt, get_greeting

if __name__ == "__main__":
    print("ğŸ¤– M.I.N.H: Powering Up...")
    memory = ConversationBufferMemory()
    greeting_message = get_greeting()
    print(f"ğŸ¤– M.I.N.H: {greeting_message}")
    while True:
        user_input = input("ğŸ‘¤ Báº¡n: ")
        confidence = exit_intent_confidence(user_input)
        if confidence in ("cao"):
            confirm_quit = input(f"ğŸ¤– M.I.N.H: {(exit_prompt())} (y/n): ").strip().lower()
            if confirm_quit in ["y", "yes", "cÃ³"]:
                print(f"ğŸ¤– M.I.N.H: {get_prompt('end')}")
                break
        # Kiá»ƒm tra náº¿u ngÆ°á»i dÃ¹ng cung cáº¥p dá»¯ liá»‡u
        data_response = provide_data_via_chat(user_input, memory)
        if data_response:
            print(f"ğŸ¤– M.I.N.H: {data_response}")
            continue
        print("ğŸ¤–M.I.N.H Ä‘ang suy nghÄ©...")
        response = ask_llm_with_memory(user_input, memory)
        print(f"ğŸ¤– M.I.N.H: {response}")