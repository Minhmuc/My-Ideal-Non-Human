from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable
from core.memory import ConversationBufferMemory
from core.prompts import get_prompt
from core.webSearch import search_web
from data.realtime_data import get_current_datetime, get_weather
from core.prompt_engineering import should_search
from core.vectorstore import search_similar

model = OllamaLLM(model="llama3.1:8b")

template = get_prompt("system") + "\n{history}\nThÃ´ng tin tÃ¬m Ä‘Æ°á»£c:\n{web_info}\nNgÆ°á»i dÃ¹ng: {question}"
prompt = ChatPromptTemplate.from_template(template)
chain: Runnable = prompt | model

def ask_llm_with_context(question: str, history: str = "", web_info: str = "") -> str:
    """Há»i LLM kÃ¨m ngá»¯ cáº£nh tá»« web."""
    custom_prompt = ChatPromptTemplate.from_template(template)
    custom_chain = custom_prompt | model
    return custom_chain.invoke({
        "question": question,
        "history": history,
        "web_info": web_info
    })

def ask_llm_with_memory(question: str, memory: ConversationBufferMemory) -> str:
    if any(kw in question.lower() for kw in ["ngÃ y máº¥y", "hÃ´m nay lÃ ", "bÃ¢y giá» lÃ ", "thá»© máº¥y"]):
        return get_current_datetime()

    if any(kw in question.lower() for kw in ["thá»i tiáº¿t", "trá»i cÃ³ mÆ°a", "nhiá»‡t Ä‘á»™", "trá»i náº¯ng khÃ´ng"]):
        return get_weather()

    history = memory.get_history()

    # ğŸ§  TÃ¬m trong Vector Store
    vector_results = search_similar(question)
    vector_info = "\n".join([doc.page_content for doc in vector_results]) if vector_results else ""

    web_info = ""
    # ğŸŒ Náº¿u nÃªn search web â†’ tÃ¬m
    if should_search(question):
        web_info = search_web(question)

    # ğŸ§  Æ¯u tiÃªn vector_info + web_info
    combined_info = f"{vector_info}\n{web_info}".strip()

    # ğŸ’¬ Há»i LLM
    answer = ask_llm_with_context(question, history, combined_info)

    # â“ Náº¿u LLM tráº£ lá»i khÃ´ng rÃµ â†’ thá»­ tÃ¬m web láº§n ná»¯a (náº¿u chÆ°a tÃ¬m)
    if (
        answer.strip().lower() in ["", "tÃ´i khÃ´ng biáº¿t.", "tÃ´i khÃ´ng rÃµ."] or
        len(answer.strip()) < 30 or
        "khÃ´ng biáº¿t" in answer.lower() or
        "khÃ´ng rÃµ" in answer.lower()
    ) and not web_info:
        web_info = search_web(question)
        combined_info = f"{vector_info}\n{web_info}".strip()
        answer = ask_llm_with_context(question, history, combined_info)

    memory.add("user", question)
    memory.add("bot", answer)

    return answer