from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable
from core.memory import ConversationBufferMemory
from core.prompts import get_prompt
from core.webSearch import search_web
from data.realtime_data import get_current_datetime, get_weather
from core.prompt_engineering import should_search, extract_location_from_question
from core.vectorstore import search_similar

model = OllamaLLM(model="llama3.1:8b")

template = get_prompt("system") + "\n{history}\nTh√¥ng tin t√¨m ƒë∆∞·ª£c:\n{web_info}\nNg∆∞·ªùi d√πng: {question}"
prompt = ChatPromptTemplate.from_template(template)
chain: Runnable = prompt | model

def ask_llm_with_context(question: str, history: str = "", web_info: str = "") -> str:
    """H·ªèi LLM k√®m ng·ªØ c·∫£nh t·ª´ web."""
    custom_prompt = ChatPromptTemplate.from_template(template)
    custom_chain = custom_prompt | model
    return custom_chain.invoke({
        "question": question,
        "history": history,
        "web_info": web_info
    })

def ask_llm_with_memory(question: str, memory: ConversationBufferMemory) -> str:
    if any(kw in question.lower() for kw in ["ng√†y m·∫•y", "h√¥m nay l√†", "b√¢y gi·ªù l√†", "th·ª© m·∫•y"]):
        return get_current_datetime()

    if any(kw in question.lower() for kw in ["th·ªùi ti·∫øt", "th·ªùi ti·∫øt h√¥m nay", "n·∫Øng", "m∆∞a"]):
        location = extract_location_from_question(question)
        return get_weather(location)


    history = memory.get_history()

    # üß† T√¨m trong Vector Store
    vector_results = search_similar(question)
    vector_info = "\n".join([doc.page_content for doc in vector_results]) if vector_results else ""

    web_info = ""
    # üåê N·∫øu n√™n search web ‚Üí t√¨m
    if should_search(question):
        web_info = search_web(question)

    # üß† ∆Øu ti√™n vector_info + web_info
    combined_info = f"{vector_info}\n{web_info}".strip()

    # üí¨ H·ªèi LLM
    answer = ask_llm_with_context(question, history, combined_info)

    # ‚ùì N·∫øu LLM tr·∫£ l·ªùi kh√¥ng r√µ ‚Üí th·ª≠ t√¨m web l·∫ßn n·ªØa (n·∫øu ch∆∞a t√¨m)
    if (
        answer.strip().lower() in ["", "t√¥i kh√¥ng bi·∫øt.", "t√¥i kh√¥ng r√µ."] or
        len(answer.strip()) < 30 or
        "kh√¥ng bi·∫øt" in answer.lower() or
        "kh√¥ng r√µ" in answer.lower()
    ) and not web_info:
        web_info = search_web(question)
        combined_info = f"{vector_info}\n{web_info}".strip()
        answer = ask_llm_with_context(question, history, combined_info)

    memory.add("user", question)
    memory.add("bot", answer)

    return answer
