from core.models import model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable
from core.memory import ConversationBufferMemory
from core.prompts import get_prompt
from core.webSearch import search_web
from data.realtime_data import get_current_datetime, get_weather
from core.prompt_engineering import should_search, extract_location_from_question, is_weather_intent, is_date_time_intent, date_time_response, weather_response
from core.vectorstore import search_similar, add_texts_to_vectorstore


template = """
ƒê√¢y l√† danh t√≠nh c·ªßa b·∫°n: {system_prompt}
D·ªØ li·ªáu li√™n quan t·ª´ h·ªá th·ªëng (n·∫øu c√≥): {retrieved_info}
C√¢u h·ªèi hi·ªán t·∫°i: {question}
Tr·∫£ l·ªùi t·ª± nhi√™n, ng·∫Øn g·ªçn, s√∫c t√≠ch v√† ch√≠nh x√°c, ∆∞u ti√™n s·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ l∆∞u n·∫øu li√™n quan. N·∫øu kh√¥ng r√µ, h√£y h·ªèi l·∫°i ng∆∞·ªùi d√πng ƒë·ªÉ l√†m r√µ.
"""
prompt = ChatPromptTemplate.from_template(template)
chain: Runnable = prompt | model

def ask_llm_with_context(question: str, history: str = "", retrieved_info: str = "") -> str:
    """H·ªèi LLM k√®m ng·ªØ c·∫£nh t·ª´ web v√† vectorstore."""
    return chain.invoke({
        "system_prompt": get_prompt("system"),
        "question": question,
        "history": history,
        "retrieved_info": retrieved_info
    })
def provide_data_via_chat(user_input: str, memory: ConversationBufferMemory) -> str:
    """
    Cho ph√©p ng∆∞·ªùi d√πng cung c·∫•p d·ªØ li·ªáu tr·ª±c ti·∫øp qua chat. N·∫øu c√¢u h·ªèi b·∫Øt ƒë·∫ßu b·∫±ng 'd·ªØ li·ªáu:' ho·∫∑c 'data:', l∆∞u n·ªôi dung v√†o vectorstore.
    """
    if user_input.lower().startswith(('d·ªØ li·ªáu:', 'data:')):
        data_content = user_input.split(':', 1)[-1].strip()
        if data_content:
            add_texts_to_vectorstore([f"D·ªØ li·ªáu ng∆∞·ªùi d√πng: {data_content}"])
            memory.add("Ng∆∞·ªùi d√πng", user_input)
            memory.add("MINH", "ƒê√£ l∆∞u d·ªØ li·ªáu c·ªßa s·∫øp v√†o h·ªá th·ªëng. S·∫øp c√≥ th·ªÉ h·ªèi l·∫°i b·∫•t c·ª© l√∫c n√†o!")
            return "ƒê√£ l∆∞u d·ªØ li·ªáu c·ªßa s·∫øp v√†o h·ªá th·ªëng. S·∫øp c√≥ th·ªÉ h·ªèi l·∫°i b·∫•t c·ª© l√∫c n√†o!"
        else:
            return "S·∫øp c·∫ßn nh·∫≠p n·ªôi dung d·ªØ li·ªáu sau 'd·ªØ li·ªáu:' ho·∫∑c 'data:' nh√©!"
    return None

def ask_llm_with_memory(question: str, memory: ConversationBufferMemory) -> str:

    history = memory.get_history()

    # üß† T√¨m trong Vector Store
    vector_results = search_similar(question, k=5)
        # N·∫øu k·∫øt qu·∫£ l√† tuple (Document, score)
    if vector_results and isinstance(vector_results[0], tuple) and len(vector_results[0]) == 2:
        vector_info = "\n".join([doc.page_content for doc, score in vector_results if score > 0.7])
        if not vector_info:
            vector_info = "\n".join([doc.page_content for doc, score in vector_results[:3]])
    else:
        vector_info = "\n".join([doc.page_content for doc in vector_results]) if vector_results else ""

    web_info = ""
    # üåê N·∫øu n√™n search web ‚Üí t√¨m
    if should_search(question):
        web_info = search_web(question)

    # üß† ∆Øu ti√™n vector_info + web_info
    retrieved_info = vector_info.strip()
    if web_info:
        retrieved_info += f"\nTh√¥ng tin m·ªõi t√¨m ki·∫øm: {web_info.strip()}"

    # üí¨ H·ªèi LLM
    answer = ask_llm_with_context(question, "", retrieved_info)

    # ‚ùì N·∫øu LLM tr·∫£ l·ªùi kh√¥ng r√µ ‚Üí th·ª≠ t√¨m web l·∫ßn n·ªØa (n·∫øu ch∆∞a t√¨m)
    if answer.strip().lower() in ["", "t√¥i kh√¥ng bi·∫øt.", "t√¥i kh√¥ng r√µ."] and not web_info:
        web_info = search_web(question)
        retrieved_info = f"{vector_info}\n{web_info}".strip()
        answer = chain.invoke({
            "system_prompt": get_prompt("system"),
            "question": question,
            "history": history,
            "retrieved_info": retrieved_info
        })

    memory.add("Ng∆∞·ªùi d√πng", question)
    memory.add("MINH", answer)
    qa_pair = f"Ng∆∞·ªùi d√πng: {question}\nMINH: {answer}"
    add_texts_to_vectorstore([qa_pair])

    return answer