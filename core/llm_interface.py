from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable
from core.memory import ConversationBufferMemory
from core.prompts import get_prompt
from core.webSearch import search_web
from data.realtime_data import get_current_datetime, get_weather
from core.prompt_engineering import should_search, extract_location_from_question, is_weather_intent, is_date_time_intent, date_time_response
from core.vectorstore import search_similar

model = OllamaLLM(model="llama3.1:8b")

template = get_prompt("system") + "\n{history}\nThÃ´ng tin tÃ¬m Ä‘Æ°á»£c:\n{web_info}\nNgÆ°á»i dÃ¹ng: {question}"
prompt = ChatPromptTemplate.from_template(template)
chain: Runnable = prompt | model

def ask_llm_with_context(question: str, history: str = "", web_info: str = "") -> str:
    """Há»i LLM kÃ¨m ngá»¯ cáº£nh tá»« web."""
    custom_prompt = ChatPromptTemplate.from_template(template)
    custom_chain = custom_prompt | model
    return custom_chain.invoke({
        "question": question,
        "history": history,
        "web_info": web_info
    })

def ask_llm_with_memory(question: str, memory: ConversationBufferMemory) -> str:
    if is_date_time_intent(question):
        return date_time_response(question, get_current_datetime())

    if is_weather_intent(question):
        location = extract_location_from_question(question)
        return get_weather(location)


    history = memory.get_history()

    # ğŸ§  TÃ¬m trong Vector Store
    vector_results = search_similar(question)
    vector_info = "\n".join([doc.page_content for doc in vector_results]) if vector_results else ""

    web_info = ""
    # ğŸŒ Náº¿u nÃªn search web â†’ tÃ¬m
    if should_search(question):
        web_info = search_web(question)

    # ğŸ§  Æ¯u tiÃªn vector_info + web_info
    combined_info = f"{vector_info}\n{web_info}".strip()

    # ğŸ’¬ Há»i LLM
    answer = ask_llm_with_context(question, history, combined_info)

    # â“ Náº¿u LLM tráº£ lá»i khÃ´ng rÃµ â†’ thá»­ tÃ¬m web láº§n ná»¯a (náº¿u chÆ°a tÃ¬m)
    if (
        answer.strip().lower() in ["", "tÃ´i khÃ´ng biáº¿t.", "tÃ´i khÃ´ng rÃµ."] or
        len(answer.strip()) < 30 or
        "khÃ´ng biáº¿t" in answer.lower() or
        "khÃ´ng rÃµ" in answer.lower()
    ) and not web_info:
        web_info = search_web(question)
        combined_info = f"{vector_info}\n{web_info}".strip()
        answer = ask_llm_with_context(question, history, combined_info)

    memory.add("user", question)
    memory.add("bot", answer)

    return answer
